{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: LLM agents & RL fine-tuning\n",
    "\n",
    "The third homework zooms in on the following skills: implementing an advanced generation system, diving into task-specific RL fine-tuning hands-on and critically thinking about fine-tuning of LMs.\n",
    "\n",
    "### Logistics\n",
    "\n",
    "* submission deadline: June 28th th 23:59 German time via Moodle\n",
    "  * please upload a **SINGLE .IPYNB FILE named Surname_FirstName_HW3.ipynb** containing your solutions of the homework.\n",
    "* please solve and submit the homework **individually**! \n",
    "* if you use Colab, to speed up the execution of the code on Colab, you can use the available GPU (if Colab resources allow). For that, before executing your code, navigate to Runtime > Change runtime type > GPU > Save.\n",
    "* please note that we will need a lot of GPU memory for both Ex. 1 and Ex. 2 -- therefore, it might be best to do the tasks in **separate runtimes on Colab**, otherwise you might run into out of memory issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Building a retrieval-augmented generation system (30 points)\n",
    "\n",
    "An increasingly popular approach to language generation is so called *retrieval-augmented generation* (RAG) wherein a language model is supplied with additional (textual) information retrieved from some storage, in addition to the actual task query. It has been found that this additional context improves model performance, and, e.g., allows to use LLMs with custom information (e.g., proprietary documents etc). \n",
    "\n",
    "The general set up of a RAG system is as follows:\n",
    "1. Some form of a database (DB) with (searchable) relevant background information (e.g., a database, a set of documents, ...) is created.\n",
    "   1. A common database format are *vector DBs*, or, vectore stores. You can optionally learn more about vector DBs, e.g., here: https://www.pinecone.io/learn/vector-database/. The important conceptual point is that some form of a searchable database with relevant (textual) information is created.\n",
    "2. An LLM that will be generating the responses to the queries, given context, is chosen.\n",
    "3. An embedding model is chosen.\n",
    "4. Task queries (e.g., questions or instructions) are provided to the system. \n",
    "   1. The query is converted to an embedding (using the model chosen ins tep 3), and the embedding is used to search and retrieve relevant information from the database. The specific retrieval method depnds on the nature of the database.\n",
    "   2. The relevant information is supplied to the LLM as context.\n",
    "5. Given the extended context, the LLM provides output.\n",
    "\n",
    "This is visualized in the figure below.\n",
    "\n",
    "![img](../tutorials/pics/basic_rag.png)\n",
    "\n",
    "The image is sourced from [here](https://docs.llamaindex.ai/en/stable/getting_started/concepts/).\n",
    "\n",
    "For more details on RAG, you can read the first part of [this](https://docs.llamaindex.ai/en/stable/getting_started/concepts/) blog post (until \"important concepts within each step\"). [Here](https://arxiv.org/pdf/2005.11401) is an optional paper about RAG, in case you want to learn more. \n",
    "\n",
    "**YOUR TASK**\n",
    "> Your task in this exercise is to explore RAG by implementing a RAG system for recipe generation. The implemented RAG system should be compared to the performance of the same model in a \"vanilla\" set-up where the model solves the task directly.\n",
    ">\n",
    "> We will use the package `LlamaIndex` and the LLM `phi-3-mini-4k-instruct` model as the backbone for the implementation. We will use the `BAAI/bge-small-en-v1.5` model as our embedding model.\n",
    "> \n",
    "> We will use unstructured data in the form of a recipe dataset `m3hrdadfi/recipe_nlg_lite`. This dataset will be indexed and it will be used to supplement information for the LLM, additionally to the query. The train split of the dataset should be used for the index, and a sample from the test dataset will be used for sampling queries with which the system will be tested. \n",
    ">\n",
    "> For this task, please complete the following steps:\n",
    "> 1. Download the dataset from Huggingface. \n",
    "> 2. Briefly familiarize yourself with the dataset.\n",
    "> 3. Briefly familiarize yourself with [this](https://docs.llamaindex.ai/en/stable/getting_started/starter_example_local/) LLamaIndex example RAG system.  \n",
    "> 4. Complete the code below (in place of \"### YOUR CODE HERE ####\"), following the instructions in the comments to build a working RAG system that will generate recipes. Note that you will have to work with the LlamaIndex documentation to complete and understand the code. Some links are already provided.\n",
    "> 5. Answer the questions at the end of the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and run in your environment / on Colab, if you haven't installed these packages yet\n",
    "# !pip install llama-index-embeddings-huggingface\n",
    "# !pip install llama-index-llms-huggingface\n",
    "# !pip install sentence-transformers\n",
    "# !pip install datasets\n",
    "# !pip install llama-index\n",
    "# !pip install \"transformers[torch]\" \n",
    "# !pip install \"huggingface_hub[inference]\"\n",
    "# !pip install accelerate bitsandbytes\n",
    "from IPython.display import clear_output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "from llama_index.core import VectorStoreIndex, Settings, Document \n",
    "# from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset from HF\n",
    "dataset = load_dataset(\"m3hrdadfi/recipe_nlg_lite\")\n",
    "# convert train split to pandas dataframe\n",
    "dataset_df = pd.DataFrame(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore\n",
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. In order to construct a VectorStorageIndex with the texts from the train dataset split, we need to \n",
    "# create list of formatted texts.\n",
    "# We want to construct texts of the form: \"Name of recipe \\n\\n ingredients \\n\\n steps\"\n",
    "\n",
    "\n",
    "texts = [ #CODE\n",
    "    f\"{row['name']}\\n\\ningredients: {row['ingredients']}\\n\\nsteps: {row['steps']}\" \n",
    "    for _, row in dataset_df.iterrows()\n",
    "]\n",
    "texts[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. We construct single Documents from the texts\n",
    "# these documents will be used to construct the vector database\n",
    "documents = [Document(text=t) for t in texts]\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. We prepare some utility functions which are required for the LLM to generate maximally accurate responses \n",
    "# this includes correctly formatting the query and the context into the prompt and special tokens\n",
    "# that are expected by the chosen LLM backbone.\n",
    "\n",
    "# we format the texts into the Phi-3 prompt format\n",
    "# See https://huggingface.co/microsoft/Phi-3-mini-4k-instruct\n",
    "# to heck here how the prompt should look like!\n",
    "def completion_to_prompt(completion):\n",
    "    \n",
    "    return f\"Instruct: {completion}\\nOutput:\" # CODE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "!pip uninstall -y accelerate bitsandbytes\n",
    "!pip install accelerate bitsandbytes\n",
    "!pip install accelerate bitsandbytes --upgrade\n",
    "!pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, the RAG building blocks are put together. Your task is to find out what the different configurations mean and correctly complete the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Save setting that are reused by our RAG system across queries\n",
    "# you can learn more about the Settings object here: https://docs.llamaindex.ai/en/stable/module_guides/supporting_modules/settings/\n",
    "\n",
    "# the embedding model is defined\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\" # CODE\n",
    ")\n",
    "\n",
    "# backbone LLM is passed to the settings\n",
    "# this is actually the model that is used to generate the response to the query, given retrieved info\n",
    "# https://docs.llamaindex.ai/en/stable/understanding/using_llms/using_llms/\n",
    "# and here: https://docs.llamaindex.ai/en/stable/module_guides/models/llms/usage_custom/\n",
    "Settings.llm = HuggingFaceLLM(\n",
    "    model_name= \"microsoft/Phi-3-mini-4k-instruct\", # CODE\n",
    "    tokenizer_name= \"microsoft/Phi-3-mini-4k-instruct\", # CODE\n",
    "    context_window=1024,\n",
    "    max_new_tokens=128,\n",
    "    generate_kwargs={\"temperature\": 0.7, \"do_sample\": True},\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    "    device_map=\"auto\",\n",
    "    model_kwargs={\"torch_dtype\": torch.float16, \"quantization_config\": BitsAndBytesConfig(load_in_8bit=True), \"trust_remote_code\": True},\n",
    ")\n",
    "print(\"Set LLM!\")\n",
    "\n",
    "# https://docs.llamaindex.ai/en/stable/module_guides/indexing/vector_store_index/\n",
    "# we create a vector store from our documents\n",
    "# here, we let the VectorStore convert the documents to nodes automatically\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents # CODE\n",
    ")\n",
    "print(\"Created index!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a single example for running a query with the RAG system, and inspecting various interesting aspects of the response generated by the model. Your task is, in the following, to set up a testing loop, which will test different queries with the RAG system and vanilla generation with the same LLM. Use the example as help. Provide comments explaning the single paramters for the following example, in place of \"### YOUR COMMENT HERE ###\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/\n",
    "# we define the query engine: generic interface that allows to ask questions over data\n",
    "query_engine = index.as_query_engine(\n",
    "    ### YOUR COMMENT HERE ###\n",
    "    response_mode=\"compact\", \n",
    "    ### YOUR COMMENT HERE ###\n",
    "    similarity_top_k=3, \n",
    "    verbose=True, \n",
    ")\n",
    "# https://docs.llamaindex.ai/en/stable/module_guides/querying/response_synthesizers/\n",
    "response = query_engine.query(\"How do I make pork chop noodle soup?\")\n",
    "print(response)\n",
    "\n",
    "for i, n in enumerate(response.source_nodes):\n",
    "    print(f\"----- Node {i} -----\")\n",
    "    print(n.node.get_content())\n",
    "    print(\"score\")\n",
    "    print(n.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing loop\n",
    "rag_responses = []\n",
    "vanilla_responses = []\n",
    "retrieved_node_texts = []\n",
    "retrieved_node_scores = []\n",
    "\n",
    "# retrieve 20 random dish names from test dataset to test the system on\n",
    "test_df = pd.DataFrame(dataset[\"test\"]).sample(20)\n",
    "test_queries = [\n",
    "    f'How do I make {r[\"name\"]}?' for\n",
    "    _, r in test_df.iterrows()\n",
    "]\n",
    "print(test_queries[:5])\n",
    "\n",
    "for query in test_queries[:5]:\n",
    "    # run the query against the RAG system\n",
    "    response_rag = query_engine.query(query) # CODE\n",
    "    rag_responses.append(str(response_rag))\n",
    "\n",
    "    # record the texts of the nodes that were retrieved for this query\n",
    "    retrieved_node_texts.append(\n",
    "        [n.node.get_content() for n in response_rag.source_nodes] #CODE\n",
    "    )\n",
    "\n",
    "    # record the scores of the texts of the retrieved nodes\n",
    "    retrieved_node_scores.append(\n",
    "        [n.score for n in response_rag.source_nodes] #CODE\n",
    "    )\n",
    "\n",
    "    # implement the \"vanilla\" (i.e., straightforward) generation of the response to the same query with the backbone LLM\n",
    "    # Hint: check the intro-to-hf sheet for examples how to generate text with an LM\n",
    "    response_vanilla = generate(\n",
    "        prompt=completion_to_prompt(query)\n",
    "    ) # CODE\n",
    " \n",
    "    vanilla_responses.append(response_vanilla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_node_scores\n",
    "test_queries[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ❌ISSUE\n",
    "\n",
    "IT WORKS UNTIL `response_vanilla = `\n",
    "AND THEN I JUST GOT STUCKED. <br>\n",
    "Below, some of my answers are from slides while some from Google)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> **Questions:**\n",
    ">\n",
    "> 1. Inspect the results of the testing. (a) How often do you prefer the RAG response over the vanilla response? (b) Do you observe differences between the RAG and vanilla responses? If yes, what are these? (c) Inpsect the retrieved documents and their scores. Do they make sense for the queries? Do the scores match your intuition about their relevance for the query? \n",
    " \n",
    "> 2. What could be advantages and disadvantages of using RAG? Name 1 each. <br>\n",
    "**Answer**: <br>\n",
    "Advantage: Responses is more accurate and relevant as we have retrieved relevant docs. <br> \n",
    "Disadvantage: require more computations and more complicated to compare.\n",
    "\n",
    "> 3. What is the difference between documents and nodes in the RAG system? <br>\n",
    "**Answer**: <br>\n",
    "Documents are the original text entries from the dataset. Nodes are the simplified versions of these documents used in the system to quickly find and compare similar information\n",
    "\n",
    "> 4. What does the embedding model do? What is the measure that underlies retrieval of relevant documents?  <br>\n",
    "**Answer**: <br>\n",
    "The embedding model converts text into dense vector representations. <br>\n",
    "Usually the measure underlying is cosine similarity.\n",
    "\n",
    "> 5. What are different response modes of the query engine? Is the chosen mode a good choice for our application? Why (not)? <br>\n",
    "**Answer**: <br>\n",
    "The query engine has different response modes, such as \"compact\" and \"verbose.\" For our application, the \"compact\" mode is a good choice because it provides short, clear recipe instructions and doesn't overwhelm users with too much information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: RLHF for summarization (15 points)\n",
    "\n",
    "In this exercise, we want to fine-tune GPT-2 to generate human-like news summaries, following a procedure that is very similar to the example of the movie review generation from [sheet 4.1](https://cogsciprag.github.io/Understanding-LLMs-course/tutorials/04a-finetuning-RL.html). The exercise is based on the paper by [Ziegler et al. (2020)](https://arxiv.org/pdf/1909.08593).\n",
    "\n",
    "To this end, we will use the following components:\n",
    "* in order to initialize the policy, we use GPT-2 that was already fine-tuned for summarization, i.e., our SFT model is [this](https://huggingface.co/Ayham/albert_gpt2_Full_summarization_cnndm)\n",
    "* as our reward model, we will use a task-specific reward signal, namely, the ROUGE score that evaluates a summary generated by a model against a human \"gold standard\" summary.\n",
    "* a dataset of CNN news texts and human-written summaries (for computing the rewards) for the fine-tuning which can be found [here](https://huggingface.co/datasets/abisee/cnn_dailymail). Please note that we will use the *validation* split because we only want to run short fine-tuning. \n",
    "\n",
    "**NOTE:** for building the datset and downloading the pretrained model, ~4GB of space will be used.\n",
    "\n",
    "> **YOUR TASK:**\n",
    ">\n",
    "> Your job for this task is to set up the PPO-based training with the package `trl`, i.e., the set up step 3 of [this](https://cdn.openai.com/instruction-following/draft-20220126f/methods.svg) figure.\n",
    "> 1. Please complete the code or insert comments what a particular line of code does below where the comments says \"#### YOUR CODE / COMMENT HERE ####\". For this and for answering the questions, you might need to dig a bit deeper into the working of proximal policy optimization (PPO), the algorithm that we are using for training. You can find relevant information, e.g., [here](https://huggingface.co/docs/trl/main/en/ppo_trainer).\n",
    "> 2. To test your implementation, you can run the training for some steps, but you are NOT required to train the full model since it will take too long.\n",
    "> 3. Answer the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install trl accelerate==0.27.2 evaluate rouge_score datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries \n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "from trl import (\n",
    "    PPOTrainer,\n",
    "    PPOConfig,\n",
    "    AutoModelForCausalLMWithValueHead\n",
    ")\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration for PPO training\n",
    "config = PPOConfig(\n",
    "    model_name=\"gavin124/gpt2-finetuned-cnn-summarization-v2\",\n",
    "    learning_rate=1.41e-5,\n",
    "    steps=250,\n",
    "    #### according to the documentation of PPOConfig (https://huggingface.co/docs/trl/main/en/ppo_trainer#trl.PPOTrainer)\n",
    "    # the batch size seems to be int = 128 ####\n",
    "    batch_size=4,\n",
    "    mini_batch_size=4,\n",
    "    #### according to the above documentatio, ppo_epochs: int = 4 ####\n",
    "    ppo_epochs=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the CNN dataset into a DataFrame and and truncate the texts to 500 tokens, because we don't want the training to be too memory heavy and we want to have \"open\" some tokens for the generation (GPT-2's context window size is 1024). Then we tokenize each text and pad it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the dataset\n",
    "def build_dataset(\n",
    "        config,\n",
    "        dataset_name=\"abisee/cnn_dailymail\"\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Build dataset for training. This builds the dataset from `load_dataset`.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (`str`):\n",
    "            The name of the dataset to be loaded.\n",
    "\n",
    "    Returns:\n",
    "        dataloader (`torch.utils.data.DataLoader`):\n",
    "            The dataloader for the dataset.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.model_name) # CODE()\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = 'left'\n",
    "    # load the datasets\n",
    "    ds = load_dataset(dataset_name, '1.0.0', split=\"validation\")\n",
    "\n",
    "    def tokenize(sample):\n",
    "        sample[\"input_ids\"] = tokenizer.encode(\n",
    "            sample[\"article\"] # CODE (hint: inspect the dataset to see how to access the input text),\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "        # get the truncated natural text, too\n",
    "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"][0])\n",
    "        sample[\"label\"] =\n",
    "        return sample\n",
    "\n",
    "    ds = ds.map(tokenize, batched=False)\n",
    "    ds.set_format(type=\"torch\")\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the dataset\n",
    "dataset = build_dataset(config)\n",
    "\n",
    "def collator(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect a sample of the dataset\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the finetuned GPT2 model with a value head and the tokenizer. We load the model twice; the first model is the one that will be optimized while the second model serves as a reference to calculate the KL-divergence from the starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name) #CODE()\n",
    "ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name) #CODE\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name) #CODE\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*AutoModelForCausalLMWithValueHead* is a model class provided by `trl` that is used for training models with RL with a *baseline*. The baseline is used as shown, e.g., on slide 76-78 of lecture 05. Specifically, the baseline is simultaneously learned during training, and learns to predict the so-called action value, namely the expected reward for generating a particular completion, given the query. This baseline is implemented as an additional (scalar output) head next to the next-token prediction head of the policy, and is called the value head. Based on the query and completion representation, it learns to predict a scalar reward which is compared to the ground truth reward from the reward model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PPOTrainer takes care of device placement and optimization later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup PPO trainer\n",
    "ppo_trainer = PPOTrainer(config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = ppo_trainer.accelerator.device\n",
    "if ppo_trainer.accelerator.num_processes == 1:\n",
    "    device = 0 if torch.cuda.is_available() else \"cpu\"  # to avoid a `pipeline` bug\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def reward_fn(\n",
    "        output: list[str],\n",
    "        original_summary: list[str]\n",
    "    ):\n",
    "    \"\"\"\n",
    "    #### YOUR COMMENT HERE ####\n",
    "    Compute the reward for generated summaries using ROUGE score.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for o, s in list(zip(output, original_summary)):\n",
    "      score = rouge.compute(predictions=[o.strip()], references=[s])[\"rouge1\"]\n",
    "      scores.append(torch.tensor(score))\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_max_length = 128\n",
    "#### YOUR COMMENT HERE: explain what kind of decoding scheme these parameters initialize ####\n",
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    \"max_new_tokens\": output_max_length\n",
    "}\n",
    "\n",
    "\n",
    "for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
    "    query_tensors = batch[\"input_ids\"]\n",
    "    query_tensors = [q.squeeze() for q in query_tensors]\n",
    "    #### Get response from gpt2\n",
    "    response_tensors = []\n",
    "    for query in query_tensors:\n",
    "        response = ppo_trainer.generate(query, **generation_kwargs)\n",
    "        response_tensors.append(response.squeeze()[-output_max_length:])\n",
    "    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
    "\n",
    "    #### Compute score with the reward_fn above\n",
    "    rewards = reward_fn(batch[\"response\"], batch[\"highlights\"]) #CODE\n",
    "\n",
    "    #### Run PPO step\n",
    "    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "    ppo_trainer.log_stats(stats, batch, rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **QUESTIONS:**\n",
    "> \n",
    "> 1. What are the three main steps in the training loop? Please name them (in descriptive words, you don't need to cite the code).<br>\n",
    "> **Answer**\n",
    "> - STEP 1: Generating Responses: The model creates responses (summaries) based on input texts (queries) using a chosen method for generating output. <br>\n",
    "> - STEP 2: Computing Rewards: The model's responses are assessed against human-written summaries using a reward function (like the ROUGE score) to measure their quality. <br>\n",
    "> - STEP 3: PPO Optimization Step: The PPO algorithm adjusts the model's parameters based on the computed rewards, aiming to enhance the model's ability to produce better summaries that resemble human-written ones. <br>\n",
    ">\n",
    "> 3. Suppose the plots below show training metrics for different runs of the summarization model training. Interpret what each of them tells us about training success; i.e., did the training go well on this run? Do we expect to get good summaries? Why? Be concise!  <br>\n",
    "> **Answer** \n",
    "> - Reward Over Time: This plot indicates the average reward (ROUGE score) per training step. Increasing rewards suggest the model is learning to produce better summaries. Flat or decreasing trends imply the model isn't improving. <br>\n",
    "> - Policy Loss: This plot shows the loss related to policy updates. Decreasing policy loss over time indicates the model is gaining confidence in generating high-reward summaries. Spikes may indicate unstable periods or exploration phases. <br>\n",
    "> - Value Loss: This plot reflects loss linked to the value function, estimating expected rewards. A downward trend suggests the model is improving its predictions. Persistent high value loss may indicate challenges in accurately estimating future rewards. <br>\n",
    ">\n",
    "> 5. We have truncated the query articles to maximally 512 tokens. Given that we are using ROUGE with respect to ground truth summaries as a reward, why might this be problematic? <br>\n",
    "> **Answer**:\n",
    "> - Truncating query articles to a maximum of 512 tokens could pose issues because it may cut out crucial context and details essential for producing precise and top-notch summaries. ROUGE evaluates the similarity between generated summaries and reference ones, so truncating could lower scores as the generated summaries might omit vital information found in the original, longer articles.\n",
    ">\n",
    ">\n",
    "> 7. [Bonus 2pts] The overall loss that is optimized during training with PPO consists of two components: the policy loss that is computed based on the completion log probability and the reward, and the value function loss which is computed based on the the predicted and received reward for a completion. These two loss components are weighed in the total loss function with the value function coefficient (`vf_coef`). Intuitively, how does it affect training if the coefficient is set to a high value?\n",
    "> **Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](data/rewards.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Aspects of fine-tuning (5 points)\n",
    "\n",
    "> Please answer the following questions. Be concise!\n",
    ">\n",
    "> 1. When assistants are trained with RLHF, they are often optimized to be helpful and harmless. However, it has been observed that the goals of being harmless and helpful at the same time may be at odds. In particular, the problem of evasive behavior has been observed for models optimized for these goals. For example, [this paper](https://arxiv.org/pdf/2212.08073.pdf) mentions this problem. In your own words, please briefly describe what evasive behavior of LLMs is, give an example, and why it is a problem. <br>\n",
    "> **Answer**:\n",
    "Evasive behaviour of LLMs is a tendency that when LLM is answering a question potentially considered as harmful, unethical, or controversial, it avoids the most direct and useful response.\n",
    "> \n",
    "> 2. What special tokens are commonly used for chat model fine-tuning, and what is their purpose? <br>\n",
    "> **Answer**: for example, `<|user|>` and `<|assistant|>`, which identify the participant's roles in the conversation.\n",
    ">\n",
    "> 3. Please name two parameter-efficient fine-tuning techniques and briefly explain one advantage of using each technique over full-scale fine-tuning. <br>\n",
    "> **Answer**:\n",
    "> LoRA (Low Rank Adaption): fine-tune the models by injecting low-rank decomposition matrices into each layer of the model <br>\n",
    "> QLoRA (Quantized LoRA): extend LoRA by applying quantization techniques to the added low-rank matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
